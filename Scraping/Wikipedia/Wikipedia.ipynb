{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18421,
     "status": "ok",
     "timestamp": 1741579485675,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "rIq10oeCCZfj",
    "outputId": "b910539d-f7df-42d0-cbaf-495698015b5f"
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install wikipedia-api\n",
    "!pip install tqdm\n",
    "!pip install random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfiXxPI1CYsc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1741584476359,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "hou3mDZUpP4u"
   },
   "outputs": [],
   "source": [
    "def get_recent_articles(url, limit=2000):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    page_count = 0\n",
    "    while len(articles) < limit:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        ul_tag = soup.find('ul', class_='mw-contributions-list')\n",
    "\n",
    "        if ul_tag is None:\n",
    "            print(\"Failed to find the list of articles.\")\n",
    "            break\n",
    "\n",
    "        for li_tag in ul_tag.find_all('li'):\n",
    "            a_tag = li_tag.find('a', class_='mw-newpages-pagename')\n",
    "            if a_tag:\n",
    "                article_title = a_tag['title']\n",
    "                article_url = 'https://en.wikipedia.org' + a_tag['href']\n",
    "                articles.append((article_title, article_url))\n",
    "                if len(articles) >= limit:\n",
    "                    break\n",
    "\n",
    "        page_count += 1\n",
    "        if page_count % 5 == 0 or len(articles) >= limit:\n",
    "            print(f\"Collected {len(articles)} articles so far...\")\n",
    "\n",
    "        next_link = soup.find('a', class_='mw-nextlink')\n",
    "        if next_link:\n",
    "            url = 'https://en.wikipedia.org' + next_link['href']\n",
    "        else:\n",
    "            print(\"Couldn't find anymore pages to scrape.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between requests\n",
    "\n",
    "    print(f\"Finished collecting {len(articles)} articles.\")\n",
    "    return articles[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1741585305883,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "VWgcIGXRYyPn"
   },
   "outputs": [],
   "source": [
    "def scrape_wikipedia_all_pages(limit=2000):\n",
    "    base_url = \"https://en.wikipedia.org/wiki/Special:AllPages\"\n",
    "    next_page = None  # For pagination\n",
    "    articles = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(articles) < limit:\n",
    "        url = base_url if not next_page else f\"https://en.wikipedia.org{next_page}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the list of article links\n",
    "        for link in soup.select(\".mw-allpages-chunk a\"):\n",
    "            title = link.text\n",
    "            article_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "            articles.append((title, article_url))\n",
    "            if len(articles) >= limit:\n",
    "                break\n",
    "\n",
    "        # Find next page link\n",
    "        next_link = soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n",
    "        if next_link:\n",
    "            next_page = next_link[\"href\"]\n",
    "        else:\n",
    "            print(\"No more pages to scrape: Couldn't find next button.\")\n",
    "            break  # Stop if there's no next page\n",
    "\n",
    "        page_count += 1\n",
    "        print(f\"✅ Scraped {len(articles)} articles so far... (Page {page_count})\")\n",
    "\n",
    "        # Random delay to avoid detection\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    print(f\"✅ Finished scraping {len(articles)} articles.\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1741586697377,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "ffzHnHoDlY7k"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import unquote  # Import URL decoding function\n",
    "\n",
    "def get_creation_date(page_url):\n",
    "    \"\"\"\n",
    "    This function uses the Wikipedia API to fetch the revision history of a page\n",
    "    and checks if the creation date is in August 2024.\n",
    "    \"\"\"\n",
    "    # Extract the page title from the URL and decode it\n",
    "    page_title = unquote(page_url.split('/')[-1])\n",
    "\n",
    "    # Wikipedia API URL for page revisions\n",
    "    api_url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': page_title,\n",
    "        'prop': 'revisions',\n",
    "        'rvlimit': 1,  # Only need the first revision (creation)\n",
    "        'rvdir': 'newer',\n",
    "        'format': 'json',\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if the page exists and has revisions\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    for page_id, page_data in pages.items():\n",
    "        if 'revisions' in page_data:\n",
    "            creation_timestamp = page_data['revisions'][0]['timestamp']\n",
    "            # Parse the timestamp to check if it falls within August 2024\n",
    "            creation_date = creation_timestamp[:7]  # Format: YYYY-MM\n",
    "            if creation_date == '2024-08':\n",
    "                print(f\"Article '{page_title}' created on {creation_date}\")  # Debugging: Print creation date\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def scrape_wikipedia_all_pages_2(limit=2000):\n",
    "    # base_url = \"https://en.wikipedia.org/wiki/Special:AllPages\"\n",
    "    base_url = \"https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=%22Ako+ang+Batas%22+-Gen.+Tomas+Karingal\"\n",
    "    next_page = None  # For pagination\n",
    "    articles = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(articles) < limit:\n",
    "        url = base_url if not next_page else f\"https://en.wikipedia.org{next_page}\"\n",
    "        print(f\"Fetching {url}\")  # Debugging: Print the URL being fetched\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the list of article links\n",
    "        for link in soup.select(\".mw-allpages-chunk a\"):\n",
    "            title = link.text\n",
    "            article_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "\n",
    "            # Check if the page was created in August 2024\n",
    "            if get_creation_date(article_url):\n",
    "                articles.append((title, article_url))\n",
    "                if len(articles) >= limit:\n",
    "                    break\n",
    "\n",
    "        # Debug: Print out the next page link\n",
    "        next_link = soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n",
    "        if next_link:\n",
    "            next_page = next_link[\"href\"]\n",
    "            print(f\"Next page found: {next_page}\")  # Debugging: Print next page link\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")  # Debugging: Stop condition\n",
    "            break  # Stop if there's no next page\n",
    "\n",
    "        page_count += 1\n",
    "        print(f\"✅ Scraped {len(articles)} articles so far... (Page {page_count})\")\n",
    "\n",
    "        # Random delay to avoid detection\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    print(f\"✅ Finished scraping {len(articles)} articles.\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1741585102044,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "-sc0epN_Chjt"
   },
   "outputs": [],
   "source": [
    "def scrape_wikipedia_article(page_title, wiki_wiki, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            page = wiki_wiki.page(page_title)\n",
    "            if not page.exists():\n",
    "                print(f\"Page '{page_title}' does not exist.\")\n",
    "                return None\n",
    "            return page.text\n",
    "        except (requests.RequestException, requests.Timeout) as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Error scraping '{page_title}'. Retrying... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            else:\n",
    "                print(f\"Failed to scrape '{page_title}' after {max_retries} attempts.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26785413,
     "status": "ok",
     "timestamp": 1741617561533,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "ztsZgwj0CkEo",
    "outputId": "4ba79471-6b54-404b-c445-32fee1d497ec"
   },
   "outputs": [],
   "source": [
    "recent_articles_url = \"https://en.wikipedia.org/wiki/Special:AllPages\"\n",
    "article_limit = 100\n",
    "print(f\"Starting to collect {article_limit} recent articles...\")\n",
    "articles = scrape_wikipedia_all_pages_2(article_limit)\n",
    "print(f\"Collected {len(articles)} articles.\")\n",
    "data = []\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='WikipediaScraper (your_email@example.com)',\n",
    "    language='en',\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "for article_title, article_url in tqdm(articles, desc=\"Scraping articles\"):\n",
    "    content = scrape_wikipedia_article(article_title, wiki_wiki)\n",
    "    if content:\n",
    "        data.append({\n",
    "            'title': article_title,\n",
    "            'url': article_url,\n",
    "            'content': content\n",
    "        })\n",
    "    time.sleep(random.uniform(0.5, 1.5))  # Random delay between article scrapes\n",
    "\n",
    "with open('scraped_wiki_articles_August_2024_1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Scraped data has been saved to 'scraped_wiki_articles.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
