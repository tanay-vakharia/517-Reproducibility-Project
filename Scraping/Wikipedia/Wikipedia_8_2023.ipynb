{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27200,
     "status": "ok",
     "timestamp": 1741634128715,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "rIq10oeCCZfj",
    "outputId": "bd4dda85-7a39-43b9-8402-43345bf65013"
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install wikipedia-api\n",
    "!pip install tqdm\n",
    "!pip install random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22064,
     "status": "ok",
     "timestamp": 1741634685745,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "ivm6I1Q-fKT4",
    "outputId": "79d17ace-6a09-4aca-c462-6aa0a97c0ff3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfiXxPI1CYsc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffzHnHoDlY7k"
   },
   "outputs": [],
   "source": [
    "def get_creation_date(page_url):\n",
    "    \"\"\"\n",
    "    This function uses the Wikipedia API to fetch the revision history of a page\n",
    "    and checks if the creation date is in August 2023.\n",
    "    \"\"\"\n",
    "    page_title = unquote(page_url.split('/')[-1])\n",
    "\n",
    "    api_url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': page_title,\n",
    "        'prop': 'revisions',\n",
    "        'rvlimit': 1,\n",
    "        'rvdir': 'newer',\n",
    "        'format': 'json',\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    for page_id, page_data in pages.items():\n",
    "        if 'revisions' in page_data:\n",
    "            creation_timestamp = page_data['revisions'][0]['timestamp']\n",
    "            creation_date = creation_timestamp[:7]\n",
    "            if creation_date == '2023-08':\n",
    "                print(f\"Article '{page_title}' created on {creation_date}\")\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def scrape_wikipedia_all_pages(limit=2000):\n",
    "    base_url = \"https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=%22Ako+ang+Batas%22+-Gen.+Tomas+Karingal\"\n",
    "    next_page = None\n",
    "    articles = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(articles) < limit:\n",
    "        url = base_url if not next_page else f\"https://en.wikipedia.org{next_page}\"\n",
    "        print(f\"Fetching {url}\")\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for link in soup.select(\".mw-allpages-chunk a\"):\n",
    "            title = link.text\n",
    "            article_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "\n",
    "            if get_creation_date(article_url):\n",
    "                articles.append((title, article_url))\n",
    "                if len(articles) >= limit:\n",
    "                    break\n",
    "\n",
    "        next_link = soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n",
    "        if next_link:\n",
    "            next_page = next_link[\"href\"]\n",
    "            print(f\"Next page found: {next_page}\")\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "\n",
    "        page_count += 1\n",
    "        print(f\"✅ Scraped {len(articles)} articles so far... (Page {page_count})\")\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    print(f\"✅ Finished scraping {len(articles)} articles.\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sc0epN_Chjt"
   },
   "outputs": [],
   "source": [
    "def scrape_wikipedia_article(page_title, wiki_wiki, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            page = wiki_wiki.page(page_title)\n",
    "            if not page.exists():\n",
    "                print(f\"Page '{page_title}' does not exist.\")\n",
    "                return None\n",
    "            return page.text\n",
    "        except (requests.RequestException, requests.Timeout) as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Error scraping '{page_title}'. Retrying... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            else:\n",
    "                print(f\"Failed to scrape '{page_title}' after {max_retries} attempts.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATGLloBEhdqL"
   },
   "outputs": [],
   "source": [
    "article_limit = 1000\n",
    "file_path = '/content/drive/My Drive/CSE 517 Project/Colabs/Scraping/Wikipedia/scraped_wiki_articles_8_2023.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59979,
     "status": "ok",
     "timestamp": 1741635266792,
     "user": {
      "displayName": "Parker Gustafson",
      "userId": "04111409094961691529"
     },
     "user_tz": 420
    },
    "id": "ztsZgwj0CkEo",
    "outputId": "d2024389-359c-49da-e942-785fd4b73c33"
   },
   "outputs": [],
   "source": [
    "print(f\"Starting to collect {article_limit} recent articles...\")\n",
    "articles = scrape_wikipedia_all_pages(article_limit)\n",
    "print(f\"Collected {len(articles)} articles.\")\n",
    "data = []\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='WikipediaScraper (your_email@example.com)',\n",
    "    language='en',\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "for article_title, article_url in tqdm(articles, desc=\"Scraping articles\"):\n",
    "    content = scrape_wikipedia_article(article_title, wiki_wiki)\n",
    "    if content:\n",
    "        data.append({\n",
    "            'title': article_title,\n",
    "            'url': article_url,\n",
    "            'content': content\n",
    "        })\n",
    "    time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Scraped data has been saved to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
