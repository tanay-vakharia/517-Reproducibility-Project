{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install requests\n","!pip install beautifulsoup4\n","!pip install wikipedia-api\n","!pip install tqdm\n","!pip install random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIq10oeCCZfj","executionInfo":{"status":"ok","timestamp":1741634128715,"user_tz":420,"elapsed":27200,"user":{"displayName":"Parker Gustafson","userId":"04111409094961691529"}},"outputId":"bd4dda85-7a39-43b9-8402-43345bf65013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n","Collecting wikipedia-api\n","  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from wikipedia-api) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2025.1.31)\n","Building wheels for collected packages: wikipedia-api\n","  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=d5fa536af686595283861b606c6ad2013dac79b4c34d9399acc6aa2bd005e043\n","  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n","Successfully built wikipedia-api\n","Installing collected packages: wikipedia-api\n","Successfully installed wikipedia-api-0.8.1\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivm6I1Q-fKT4","executionInfo":{"status":"ok","timestamp":1741634685745,"user_tz":420,"elapsed":22064,"user":{"displayName":"Parker Gustafson","userId":"04111409094961691529"}},"outputId":"79d17ace-6a09-4aca-c462-6aa0a97c0ff3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import wikipediaapi\n","import json\n","from tqdm import tqdm\n","import time\n","import random\n","from urllib.parse import unquote"],"metadata":{"id":"CfiXxPI1CYsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_creation_date(page_url):\n","    \"\"\"\n","    This function uses the Wikipedia API to fetch the revision history of a page\n","    and checks if the creation date is in August 2023.\n","    \"\"\"\n","    page_title = unquote(page_url.split('/')[-1])\n","\n","    api_url = f\"https://en.wikipedia.org/w/api.php\"\n","    params = {\n","        'action': 'query',\n","        'titles': page_title,\n","        'prop': 'revisions',\n","        'rvlimit': 1,\n","        'rvdir': 'newer',\n","        'format': 'json',\n","    }\n","\n","    response = requests.get(api_url, params=params)\n","    data = response.json()\n","\n","    pages = data.get('query', {}).get('pages', {})\n","    for page_id, page_data in pages.items():\n","        if 'revisions' in page_data:\n","            creation_timestamp = page_data['revisions'][0]['timestamp']\n","            creation_date = creation_timestamp[:7]\n","            if creation_date == '2023-08':\n","                print(f\"Article '{page_title}' created on {creation_date}\")\n","                return True\n","\n","    return False\n","\n","\n","def scrape_wikipedia_all_pages(limit=2000):\n","    base_url = \"https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=%22Ako+ang+Batas%22+-Gen.+Tomas+Karingal\"\n","    next_page = None\n","    articles = []\n","    page_count = 0\n","\n","    while len(articles) < limit:\n","        url = base_url if not next_page else f\"https://en.wikipedia.org{next_page}\"\n","        print(f\"Fetching {url}\")\n","        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","\n","        try:\n","            response = requests.get(url, headers=headers, timeout=10)\n","            response.raise_for_status()\n","        except requests.RequestException as e:\n","            print(f\"Error fetching {url}: {e}\")\n","            time.sleep(5)\n","            continue\n","\n","        soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","        for link in soup.select(\".mw-allpages-chunk a\"):\n","            title = link.text\n","            article_url = f\"https://en.wikipedia.org{link['href']}\"\n","\n","            if get_creation_date(article_url):\n","                articles.append((title, article_url))\n","                if len(articles) >= limit:\n","                    break\n","\n","        next_link = soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n","        if next_link:\n","            next_page = next_link[\"href\"]\n","            print(f\"Next page found: {next_page}\")\n","        else:\n","            print(\"No more pages to scrape.\")\n","            break\n","\n","        page_count += 1\n","        print(f\"✅ Scraped {len(articles)} articles so far... (Page {page_count})\")\n","\n","        time.sleep(random.uniform(1, 3))\n","\n","    print(f\"✅ Finished scraping {len(articles)} articles.\")\n","    return articles\n"],"metadata":{"id":"ffzHnHoDlY7k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scrape_wikipedia_article(page_title, wiki_wiki, max_retries=3):\n","    for attempt in range(max_retries):\n","        try:\n","            page = wiki_wiki.page(page_title)\n","            if not page.exists():\n","                print(f\"Page '{page_title}' does not exist.\")\n","                return None\n","            return page.text\n","        except (requests.RequestException, requests.Timeout) as e:\n","            if attempt < max_retries - 1:\n","                print(f\"Error scraping '{page_title}'. Retrying... (Attempt {attempt + 1}/{max_retries})\")\n","                time.sleep(random.uniform(2, 5))\n","            else:\n","                print(f\"Failed to scrape '{page_title}' after {max_retries} attempts.\")\n","                return None"],"metadata":{"id":"-sc0epN_Chjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["article_limit = 1000\n","file_path = '/content/drive/My Drive/CSE 517 Project/Colabs/Scraping/Wikipedia/scraped_wiki_articles_8_2023.json'"],"metadata":{"id":"ATGLloBEhdqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Starting to collect {article_limit} recent articles...\")\n","articles = scrape_wikipedia_all_pages(article_limit)\n","print(f\"Collected {len(articles)} articles.\")\n","data = []\n","\n","wiki_wiki = wikipediaapi.Wikipedia(\n","    user_agent='WikipediaScraper (your_email@example.com)',\n","    language='en',\n","    timeout=30\n",")\n","\n","for article_title, article_url in tqdm(articles, desc=\"Scraping articles\"):\n","    content = scrape_wikipedia_article(article_title, wiki_wiki)\n","    if content:\n","        data.append({\n","            'title': article_title,\n","            'url': article_url,\n","            'content': content\n","        })\n","    time.sleep(random.uniform(0.5, 1.5))\n","\n","with open(file_path, 'w', encoding='utf-8') as f:\n","    json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","print(f\"Scraped data has been saved to {file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztsZgwj0CkEo","outputId":"d2024389-359c-49da-e942-785fd4b73c33","executionInfo":{"status":"ok","timestamp":1741635266792,"user_tz":420,"elapsed":59979,"user":{"displayName":"Parker Gustafson","userId":"04111409094961691529"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting to collect 2 recent articles...\n","Fetching https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=%22Ako+ang+Batas%22+-Gen.+Tomas+Karingal\n","Article '\"Casual\"_(Chappell_Roan_song)' created on 2024-08\n","Article '\"Checkers\"_Speech' created on 2024-08\n","Next page found: /w/index.php?title=Special:AllPages&from=%22Chisinau%22+airport\n","✅ Scraped 2 articles so far... (Page 1)\n","✅ Finished scraping 2 articles.\n","Collected 2 articles.\n"]},{"output_type":"stream","name":"stderr","text":["Scraping articles: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Scraped data has been saved to /content/drive/My Drive/CSE 517 Project/Colabs/Scraping/Wikipedia/scraped_wiki_articles_TEST.json\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}