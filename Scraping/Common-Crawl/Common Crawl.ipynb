{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741755933232,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "lcsrGA73Xznd"
   },
   "outputs": [],
   "source": [
    "# Define the specific crawls we want to use\n",
    "TARGET_CRAWLS = [\n",
    "    # These IDs will be updated based on what's available when running the code\n",
    "    # Format: {\"name\": \"descriptive_name\", \"id\": \"CC-MAIN-YYYY-WW\"}\n",
    "    {\"name\": \"February 2025\", \"id\": \"CC-MAIN-2025-08\"},  # February 2025 (or closest available)\n",
    "    {\"name\": \"August 2024\", \"id\": \"CC-MAIN-2024-33\"},    # August 2024 (or closest available)\n",
    "    {\"name\": \"Fall 2023\", \"id\": \"CC-MAIN-2023-40\"},      # September/October 2023 (or closest available)\n",
    "    {\"name\": \"February 2022\", \"id\": \"CC-MAIN-2022-05\"}   # February 2022 (or closest available)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5029,
     "status": "ok",
     "timestamp": 1741755938265,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "r_5K3HhLg1-e",
    "outputId": "b8372f82-c1a2-4d6a-f0ab-758a4955842b"
   },
   "outputs": [],
   "source": [
    "!pip install warc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1741755938306,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "_aRz95H8hpYB"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import re\n",
    "\n",
    "def stream_warc_paths(url, num_lines=50):\n",
    "    \"\"\"\n",
    "    Stream a gzipped file of WARC paths from Common Crawl, decompress it on the fly,\n",
    "    and return the first n paths.\n",
    "\n",
    "    Args:\n",
    "        url: URL to the gzipped paths file\n",
    "        num_lines: Number of paths to return (default: 50)\n",
    "\n",
    "    Returns:\n",
    "        List of the first num_lines WARC file paths\n",
    "    \"\"\"\n",
    "    # Stream the response without downloading the whole file\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download file: {response.status_code}\")\n",
    "\n",
    "    # Create a gzip decompressor that can process chunks of data\n",
    "    decompressor = gzip.GzipFile(fileobj=io.BytesIO(response.content), mode='rb')\n",
    "\n",
    "    # Read and decode the first num_lines lines\n",
    "    lines = []\n",
    "    for i, line in enumerate(decompressor):\n",
    "        if i >= num_lines:\n",
    "            break\n",
    "        lines.append(line.decode('utf-8').strip())\n",
    "\n",
    "    return lines\n",
    "\n",
    "def stream_warc_content(warc_path, base_url=\"https://data.commoncrawl.org/\", max_records=5):\n",
    "    \"\"\"\n",
    "    Stream a specific WARC file from Common Crawl, decompress it on the fly,\n",
    "    and extract the first few web page contents.\n",
    "\n",
    "    Args:\n",
    "        warc_path: Path to WARC file (from warc.paths.gz)\n",
    "        base_url: Base URL for Common Crawl data\n",
    "        max_records: Maximum number of WARC records to process\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with WARC record information\n",
    "    \"\"\"\n",
    "    full_url = base_url + warc_path\n",
    "    print(f\"Retrieving WARC file from: {full_url}\")\n",
    "\n",
    "    # Stream the WARC.gz file\n",
    "    response = requests.get(full_url, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download WARC file: {response.status_code}\")\n",
    "\n",
    "    # Process the compressed data in chunks to avoid memory issues\n",
    "    # We'll use a small buffer size to demonstrate streaming\n",
    "    buffer_size = 1024 * 1024  # 1MB chunks\n",
    "\n",
    "    records = []\n",
    "    record_count = 0\n",
    "\n",
    "    # Simple WARC parsing\n",
    "    in_header = False\n",
    "    current_headers = {}\n",
    "    current_content = []\n",
    "    in_record = False  # Flag to track if we're inside a record\n",
    "\n",
    "    # Create a stream reader for the gzipped content\n",
    "    stream = io.BytesIO(response.content)\n",
    "    decompressor = gzip.GzipFile(fileobj=stream, mode='rb')\n",
    "\n",
    "    # Buffer for incomplete lines\n",
    "    buffer = b\"\"\n",
    "\n",
    "    # Process decompressed data line by line\n",
    "    while True:\n",
    "        # Read a chunk of data\n",
    "        chunk = decompressor.read(buffer_size)\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        # Combine with any leftover buffer from previous chunk\n",
    "        data = buffer + chunk\n",
    "\n",
    "        # Split into lines, keeping the last potentially incomplete line in the buffer\n",
    "        lines = data.split(b'\\r\\n')\n",
    "        buffer = lines.pop() if lines else b\"\"\n",
    "\n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            line_str = line.decode('utf-8', errors='ignore')\n",
    "\n",
    "            # WARC record separator - start of a new record\n",
    "            if line_str.startswith('WARC/1.0'):\n",
    "                # Save previous record if it exists and we haven't reached max_records\n",
    "                if current_headers and record_count < max_records and in_record:\n",
    "                    records.append({\n",
    "                        'headers': current_headers,\n",
    "                        'content': ''.join(current_content)\n",
    "                    })\n",
    "                    record_count += 1\n",
    "\n",
    "                # Start a new record\n",
    "                current_headers = {}\n",
    "                current_content = []\n",
    "                in_header = True\n",
    "                in_record = True\n",
    "                continue\n",
    "\n",
    "            # Empty line marks the end of headers\n",
    "            if in_header and not line_str:\n",
    "                in_header = False\n",
    "                continue\n",
    "\n",
    "            # Process header lines\n",
    "            if in_header:\n",
    "                if ': ' in line_str:\n",
    "                    key, value = line_str.split(': ', 1)\n",
    "                    current_headers[key] = value\n",
    "            else:\n",
    "                # Add to content if we're in a record's body\n",
    "                if current_headers:\n",
    "                    current_content.append(line_str + '\\n')\n",
    "\n",
    "            # Check if we've reached max_records\n",
    "            if record_count >= max_records:\n",
    "                break\n",
    "\n",
    "        # Break out of the outer loop if we've reached max_records\n",
    "        if record_count >= max_records:\n",
    "            break\n",
    "\n",
    "    # Add the last record if we haven't reached max_records\n",
    "    if current_headers and record_count < max_records:\n",
    "        records.append({\n",
    "            'headers': current_headers,\n",
    "            'content': ''.join(current_content)\n",
    "        })\n",
    "\n",
    "    return records\n",
    "\n",
    "def extract_html_content(warc_record):\n",
    "    \"\"\"\n",
    "    Extract HTML content from a WARC response record.\n",
    "\n",
    "    Args:\n",
    "        warc_record: Dictionary containing WARC record data\n",
    "\n",
    "    Returns:\n",
    "        HTML content or empty string if no HTML content is found\n",
    "    \"\"\"\n",
    "    content = warc_record['content']\n",
    "\n",
    "    # Check if it's a response record\n",
    "    if warc_record['headers'].get('WARC-Type') != 'response':\n",
    "        return \"\"\n",
    "\n",
    "    # In a WARC response record, the structure is:\n",
    "    # 1. WARC headers (already separated)\n",
    "    # 2. HTTP headers\n",
    "    # 3. Blank line (\\r\\n\\r\\n)\n",
    "    # 4. HTTP body (HTML content)\n",
    "\n",
    "    # First, look for the double newline that separates HTTP headers from body\n",
    "    patterns = ['\\r\\n\\r\\n', '\\n\\n']\n",
    "    for pattern in patterns:\n",
    "        # We might need to find the second occurrence if there are multiple header blocks\n",
    "        parts = content.split(pattern, 1)\n",
    "        if len(parts) > 1:\n",
    "            # If we found HTTP headers, return everything after them\n",
    "            return parts[1]\n",
    "\n",
    "    # If we can't clearly separate headers, look for HTML start\n",
    "    html_indicators = ['<!DOCTYPE', '<html', '<HTML', '<?xml']\n",
    "    for indicator in html_indicators:\n",
    "        pos = content.find(indicator)\n",
    "        if pos != -1:\n",
    "            return content[pos:]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1741755938320,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "5xQv5aIkumvb",
    "outputId": "04aae875-48ef-4f25-e5a7-7218a0311968"
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for crawl in TARGET_CRAWLS:\n",
    "    urls.append(f\"https://data.commoncrawl.org/crawl-data/{crawl['id']}/warc.paths.gz\")\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211171,
     "status": "ok",
     "timestamp": 1741756240206,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "B28KvkuLja_t",
    "outputId": "559e55aa-78e9-41a8-daff-1e54b5fb4dc5"
   },
   "outputs": [],
   "source": [
    "num_lines = 4\n",
    "limit = 1200\n",
    "crawl_data = {}\n",
    "\n",
    "try:\n",
    "    # Get first 5 WARC paths\n",
    "\n",
    "\n",
    "    for warc_paths_url in urls:\n",
    "        sites = []\n",
    "        warc_paths = stream_warc_paths(warc_paths_url, num_lines=num_lines)\n",
    "        for i, path in enumerate(warc_paths):\n",
    "            print(f\"{i+1}: {path}\")\n",
    "\n",
    "        # Now, get the content from the first WARC file\n",
    "        if warc_paths:\n",
    "            for i in range(num_lines):\n",
    "                # Only process first 2 records to keep the output manageable\n",
    "                warc_records = stream_warc_content(warc_paths[i], max_records=limit)\n",
    "                # print(f\"\\nExtracted {len(warc_records)} records from first WARC file:\")\n",
    "                for i, record in enumerate(warc_records):\n",
    "                    # print(f\"\\nRecord {i+1}:\")\n",
    "                    # print(f\"WARC Type: {record['headers'].get('WARC-Type', 'unknown')}\")\n",
    "                    # print(f\"Target URI: {record['headers'].get('WARC-Target-URI', 'unknown')}\")\n",
    "\n",
    "                    # For response records, try to extract the HTML content\n",
    "                    if record['headers'].get('WARC-Type') == 'response':\n",
    "                        html_content = extract_html_content(record)\n",
    "                        # Print a sample of the content (first 500 chars)\n",
    "                        content_preview = html_content[:500] + \"...\" if len(html_content) > 500 else html_content\n",
    "                        if len(sites) < limit: sites.append(html_content)\n",
    "                        # print(f\"HTML Content Preview:\\n{content_preview}\")\n",
    "                    else:\n",
    "                        # Print a sample of the content (first 500 chars)\n",
    "                        content_preview = record['content'][:500] + \"...\" if len(record['content']) > 500 else record['content']\n",
    "                        # print(f\"Content Preview:\\n{content_preview}\")\n",
    "\n",
    "                    # print(f\"num sites: {len(sites)}\")\n",
    "                    if len(sites) == limit: break\n",
    "                if len(sites) == limit: break\n",
    "            crawl_data[warc_paths_url] = sites\n",
    "            print(f\"=== {warc_paths_url} scraping complete. {len(sites)} sites catalogued\")\n",
    "        else:\n",
    "            print(\"No WARC paths found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIiuI6mCkY3l"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extract plain text from HTML content, removing all HTML tags.\n",
    "\n",
    "    Args:\n",
    "        html_content: String containing HTML content\n",
    "\n",
    "    Returns:\n",
    "        Plain text extracted from the HTML\n",
    "    \"\"\"\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements - they contain text we don't want\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Break into lines and remove leading/trailing whitespace\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "    # Break multi-headlines into a single line\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "    # Drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "# For use in a Colab notebook:\n",
    "# 1. Install BeautifulSoup if you haven't already\n",
    "# !pip install beautifulsoup4\n",
    "\n",
    "# 2. Use it with your HTML content\n",
    "for i, url in enumerate(crawl_data):\n",
    "    i = 1\n",
    "    print(url)\n",
    "    for site in crawl_data[url]:\n",
    "        print(f\"\\n~~~~ SITE {i} ~~~~\")\n",
    "        html_content = site\n",
    "        text = extract_text_from_html(html_content)\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1741756868955,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "KCtggWktn_LB"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extract plain text from HTML content, removing all HTML tags.\n",
    "\n",
    "    Args:\n",
    "        html_content: String containing HTML content\n",
    "\n",
    "    Returns:\n",
    "        Plain text extracted from the HTML\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Remove script and style elements - they contain text we don't want\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "\n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Break into lines and remove leading/trailing whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "        # Break multi-headlines into a single line\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def sanitize_filename(url):\n",
    "    \"\"\"\n",
    "    Convert a URL into a safe filename.\n",
    "\n",
    "    Args:\n",
    "        url: URL string\n",
    "\n",
    "    Returns:\n",
    "        Safe filename string\n",
    "    \"\"\"\n",
    "    print(url)\n",
    "    len_pre = len(\"https://data.commoncrawl.org/crawl-data/\")\n",
    "    len_post = len(\"/warc.paths.gz\")\n",
    "    filename = url[len_pre: len(url) - len_post]\n",
    "\n",
    "    # Remove protocol and replace special characters\n",
    "    filename = re.sub(r'^https?://', '', filename)\n",
    "    # Replace any character that's not alphanumeric, underscore, or dash with underscore\n",
    "    filename = re.sub(r'[^\\w\\-]', '_', filename)\n",
    "    # Truncate if too long\n",
    "    if len(filename) > 100:\n",
    "        filename = filename[:100]\n",
    "    return filename + '.json'\n",
    "\n",
    "def process_crawl_data(crawl_data, output_dir='crawl_json'):\n",
    "    \"\"\"\n",
    "    Process the crawl data and save each crawl to a separate JSON file.\n",
    "\n",
    "    Args:\n",
    "        crawl_data: Dictionary with URLs as keys and lists of HTML content as values\n",
    "        output_dir: Directory to save the JSON files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process each crawl URL\n",
    "    for url, sites in crawl_data.items():\n",
    "        # Create a JSON object for this crawl\n",
    "        crawl_json = {\n",
    "            \"crawl_url\": url,\n",
    "            \"sites\": []\n",
    "        }\n",
    "\n",
    "        print(f\"Processing crawl: {url}\")\n",
    "\n",
    "        # Process each site in this crawl\n",
    "        for i, html_content in enumerate(sites):\n",
    "            print(f\"  Processing site {i+1}/{len(sites)}\")\n",
    "\n",
    "            # Extract text from HTML\n",
    "            text = extract_text_from_html(html_content)\n",
    "\n",
    "            # Add site data to the JSON object\n",
    "            site_data = {\n",
    "                \"site_id\": i+1,\n",
    "                \"html_length\": len(html_content),\n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "            crawl_json[\"sites\"].append(site_data)\n",
    "\n",
    "        # Create filename from URL\n",
    "        filename = sanitize_filename(url)\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Write JSON to file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(crawl_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Saved crawl data to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184834,
     "status": "ok",
     "timestamp": 1741757104413,
     "user": {
      "displayName": "Phil Bedford",
      "userId": "17423175767115438447"
     },
     "user_tz": 420
    },
    "id": "xB74SySWzDNb",
    "outputId": "4edfa3c5-ddda-49c9-ab5d-fe08dc67a9c7"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to organize sites by crawl URL\n",
    "organized_data = {}\n",
    "for i, url in enumerate(crawl_data):\n",
    "    # Initialize the list for this URL if it doesn't exist\n",
    "    if url not in organized_data:\n",
    "        organized_data[url] = []\n",
    "\n",
    "    # Add all sites for this URL\n",
    "    for site in crawl_data[url]:\n",
    "        organized_data[url].append(site)\n",
    "\n",
    "# Process the organized data\n",
    "process_crawl_data(organized_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
